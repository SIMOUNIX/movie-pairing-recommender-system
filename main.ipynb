{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlretrieve\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import sys\n",
    "from shutil import rmtree\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('utils'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from data_loader import load_imdb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "## Load IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_basics, title_basics, title_ratings = load_imdb_dataset()\n",
    "\n",
    "print(name_basics.shape)\n",
    "print(title_basics.shape)\n",
    "print(title_ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MovieLens 1M dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "\n",
    "if (not os.path.exists(data_path)):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "# Remove ml-1m if it exists in the data folder\n",
    "if 'ml-1m' in os.listdir(data_path):\n",
    "    rmtree(data_path + 'ml-1m')\n",
    "    \n",
    "dsURL = \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\", \"movielens.zip\";\n",
    "print(f\"Downloading {dsURL[0]}...\")\n",
    "urlretrieve(dsURL[0], dsURL[1])\n",
    "\n",
    "ZipFile(dsURL[1], \"r\").extractall(data_path)\n",
    "\n",
    "# Remove the zip file\n",
    "os.remove(dsURL[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(\n",
    "    data_path + 'ml-1m/users.dat', \n",
    "    sep='::',\n",
    "    names=[\"user_id\", \"sex\", \"age_group\", \"occupation\", \"zip_code\"],\n",
    "    engine=\"python\"\n",
    ")\n",
    "\n",
    "ratings = pd.read_csv(\n",
    "    data_path + \"ml-1m/ratings.dat\",\n",
    "    sep=\"::\",\n",
    "    names=[\"user_id\", \"movie_id\", \"rating\", \"unix_timestamp\"],\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "movies = pd.read_csv(\n",
    "    data_path + \"ml-1m/movies.dat\",\n",
    "    sep=\"::\",\n",
    "    names=[\"movie_id\", \"title\", \"genres\"],\n",
    "    engine=\"python\",\n",
    "    encoding=\"latin-1\",\n",
    ")\n",
    "\n",
    "print(\"Size for users:\", users.shape)\n",
    "print(\"Size for ratings:\", ratings.shape)\n",
    "print(\"Size for movies:\", movies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy that we will use throughout the project\n",
    "\n",
    "ml_users = users.copy()\n",
    "ml_ratings = ratings.copy()\n",
    "ml_movies = movies.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "For this project we use two datasets that are not link to each other. We will preprocess them separately and then merge them together. What we do on the datasets:\n",
    "- change column names\n",
    "- remove columns that are not needed\n",
    "- process titles to remove unwanted characters to make the merging easier\n",
    "- genre are hot encoded\n",
    "\n",
    "## Preprocess MovieLens 1M dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_users[\"user_id\"] = ml_users[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
    "ml_users[\"age_group\"] = ml_users[\"age_group\"].apply(lambda x: f\"group_{x}\")\n",
    "ml_users[\"occupation\"] = ml_users[\"occupation\"].apply(lambda x: f\"occupation_{x}\")\n",
    "\n",
    "ml_movies[\"movie_id\"] = ml_movies[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
    "ml_movies[\"date\"] = ml_movies[\"title\"].apply(lambda x: x[-5:-1])\n",
    "ml_movies[\"title\"] = ml_movies[\"title\"].apply(lambda x: x[:-7])\n",
    "ml_movies[\"original_title\"] = ml_movies[\"title\"].str.extract(r\"\\((.*)\\)\").to_string()\n",
    "ml_movies[\"title\"] = ml_movies[\"title\"].str.replace(r\"\\(.*\\)\", \"\", regex=True).str.strip()\n",
    "\n",
    "# For all the movies title that have \", The\" or \", Les\" at the end, we will move it to the beginning without the comma. End remove it from the end.\n",
    "ml_movies[\"title\"] = ml_movies[\"title\"].apply(lambda x: \"The \" + x[:-5] if x[-5:] == \", The\" else x)\n",
    "ml_movies[\"title\"] = ml_movies[\"title\"].apply(lambda x: \"Les \" + x[:-5] if x[-5:] == \", Les\" else x)\n",
    "\n",
    "# Rename movies['title'] to movies['primary_title']\n",
    "ml_movies.rename(columns={\"title\": \"primary_title\"}, inplace=True)\n",
    "\n",
    "ml_ratings[\"movie_id\"] = ml_ratings[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
    "ml_ratings[\"user_id\"] = ml_ratings[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
    "ml_ratings[\"rating\"] = ml_ratings[\"rating\"].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_movies[\"primary_title_processed\"] = ml_movies[\"primary_title\"].apply(lambda x: x.replace(\" \", \"\").lower())\n",
    "ml_movies[\"primary_title_processed\"] = ml_movies[\"primary_title_processed\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "# If original_title is NaN, we will use the primary_title\n",
    "ml_movies[\"original_title\"] = ml_movies[\"original_title\"].fillna(ml_movies[\"primary_title\"])\n",
    "ml_movies[\"original_title_processed\"] = ml_movies[\"original_title\"].apply(lambda x: x.replace(\" \", \"\").lower())\n",
    "ml_movies[\"original_title_processed\"] = ml_movies[\"original_title_processed\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = []\n",
    "for genre in ml_movies[\"genres\"].str.split(\"|\"):\n",
    "    genres.extend(genre)\n",
    "    \n",
    "genres = list(set(genres))\n",
    "\n",
    "for genre in genres:\n",
    "    ml_movies[genre] = ml_movies[\"genres\"].apply(lambda gs: int(genre in gs.split(\"|\")))\n",
    "    \n",
    "ml_movies.drop(columns=[\"genres\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess IMDB dataset\n",
    "\n",
    "We only keep the movies, shorts and tvSeries. I personnaly do not mind being recommended a short or a tvSeries even though I am looking for a movie. I think it can be interesting to have a mix of different types of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy that we will use throughout the project\n",
    "\n",
    "imdb_name = name_basics.copy()\n",
    "imdb_title = title_basics.copy()\n",
    "imdb_rating = title_ratings.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all different titleType \n",
    "print(\"Different genres:\", imdb_title[\"titleType\"].unique())\n",
    "\n",
    "# Print the number of each titleType\n",
    "print(\"Number of each titleType:\")\n",
    "print(imdb_title[\"titleType\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_before = imdb_title.shape[0]\n",
    "\n",
    "imdb_title = imdb_title[imdb_title[\"titleType\"].isin([\"movie\", \"short\", \"tvSeries\"])]\n",
    "\n",
    "size_after = imdb_title.shape[0]\n",
    "print(f\"Number of rows removed: {size_before - size_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename \n",
    "imdb_title.rename(columns={\"primaryTitle\": \"primary_title\", \"originalTitle\": \"original_title\", \"startYear\": \"date\"}, inplace=True)\n",
    "\n",
    "# Drop columns isAdult, endYear, runtimeMinutes and titleType\n",
    "imdb_title.drop(columns=[\"isAdult\", \"endYear\", \"runtimeMinutes\", \"titleType\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = []\n",
    "for genre in imdb_title[\"genres\"].str.split(\",\"):\n",
    "    genres.extend(genre)\n",
    "    \n",
    "genres = list(set(genres))\n",
    "\n",
    "for genre in genres:\n",
    "    imdb_title[genre] = imdb_title[\"genres\"].apply(lambda gs: int(genre in gs.split(\",\") if type(gs) == str else False))\n",
    "    \n",
    "imdb_title.drop(columns=[\"genres\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column \"\\N\"\n",
    "\n",
    "imdb_title.drop(columns=[\"\\\\N\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_title[\"primary_title_processed\"] = imdb_title[\"primary_title\"].apply(lambda x: str(x).replace(\" \", \"\").lower())\n",
    "imdb_title[\"primary_title_processed\"] = imdb_title[\"primary_title_processed\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "imdb_title[\"original_title_processed\"] = imdb_title[\"original_title\"].apply(lambda x: str(x).replace(\" \", \"\").lower())\n",
    "imdb_title[\"original_title_processed\"] = imdb_title[\"original_title_processed\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge MovieLens 1M and IMDB datasets\n",
    "\n",
    "We want to merge both dataset to **enrich** the MovieLens 1M dataset with the IMDB dataset. We will the primary title of the movie. We will also use the year of the movie to filter out the movies to differentiate between movies with the same title.\n",
    "\n",
    "To do so we will :\n",
    "- Merge the datasets on the primary title of the movie (processed)\n",
    "- Process the unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_movies = pd.merge(ml_movies, imdb_title, on=['primary_title_processed', 'date'], how='left')\n",
    "\n",
    "# Remove the rows where tconst is NaN\n",
    "merged_movies = merged_movies[~merged_movies[\"tconst\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_non_zero(col1, col2):\n",
    "    if col1.name.endswith('_x'):\n",
    "        base_name = col1.name[:-2]\n",
    "    else:\n",
    "        base_name = col2.name[:-2]\n",
    "    \n",
    "    mask = (col1 != 0) | (col2 != 0)\n",
    "    result = pd.Series(0, index=col1.index, name=base_name)\n",
    "    result[mask] = col1[mask].combine_first(col2[mask])\n",
    "    return result\n",
    "\n",
    "# Identify columns with _x and _y suffixes\n",
    "x_cols = [col for col in merged_movies.columns if col.endswith('_x')]\n",
    "y_cols = [col for col in merged_movies.columns if col.endswith('_y')]\n",
    "\n",
    "# Combine the columns and remove suffixes\n",
    "for x_col in x_cols:\n",
    "    base_name = x_col[:-2]\n",
    "    y_col = base_name + '_y'\n",
    "    \n",
    "    if y_col in y_cols:\n",
    "        merged_movies[base_name] = select_non_zero(merged_movies[x_col], merged_movies[y_col])\n",
    "        merged_movies.drop(columns=[x_col, y_col], inplace=True)\n",
    "    else:\n",
    "        merged_movies.rename(columns={x_col: base_name}, inplace=True)\n",
    "\n",
    "# Rename any remaining _y columns\n",
    "for y_col in y_cols:\n",
    "    if y_col in merged_movies.columns:\n",
    "        base_name = y_col[:-2]\n",
    "        merged_movies.rename(columns={y_col: base_name}, inplace=True)\n",
    "\n",
    "# Remove any duplicate columns that might still exist\n",
    "merged_movies = merged_movies.loc[:, ~merged_movies.columns.duplicated()]\n",
    "\n",
    "print(\"Merged and cleaned dataset shape:\", merged_movies.shape)\n",
    "print(\"Columns in final dataset:\", merged_movies.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the columns for better readability\n",
    "\n",
    "new_order = [\n",
    "    'movie_id', 'tconst', \n",
    "    \n",
    "    'primary_title', 'original_title', \n",
    "    \n",
    "    'date',\n",
    "    \n",
    "    'Action', 'Adventure', 'Animation', 'Biography', 'Children\\'s', 'Comedy', 'Crime', \n",
    "    'Documentary', 'Drama', 'Family', 'Fantasy', 'Film-Noir', 'Game-Show', 'History', \n",
    "    'Horror', 'Music', 'Musical', 'Mystery', 'News', 'Reality-TV', 'Romance', 'Sci-Fi', \n",
    "    'Short', 'Sport', 'Talk-Show', 'Thriller', 'War', 'Western', 'Adult'\n",
    "]\n",
    "\n",
    "merged_movies = merged_movies[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the float columns to int\n",
    "for col in merged_movies.columns:\n",
    "    if merged_movies[col].dtype == float:\n",
    "        merged_movies[col] = merged_movies[col].fillna(0).astype(int)\n",
    "        \n",
    "display(merged_movies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This merging will allow us to have more information about the movies in the MovieLens 1M dataset. We only kept the columns that are useful for the recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction\n",
    "\n",
    "For this exercise we will use the following features:\n",
    "- genres\n",
    "- actors and actresses\n",
    "- average rating from IMDB\n",
    "- user rating from MovieLens 1M\n",
    "\n",
    "I decided to use the actors and actresses as features because based on my girlfriend and I experience, we tend to like movies with the same actors and actresses. We sometimes watch a movie because of the casting and not the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_data = imdb_name[imdb_name['primaryProfession'].str.contains('actor|actress', case=False, na=False)]\n",
    "\n",
    "# Dictionary mapping tconst to a list of actors\n",
    "movie_actors = {}\n",
    "for _, row in actor_data.iterrows():\n",
    "    for movie in row['knownForTitles'].split(','):\n",
    "        if movie in movie_actors:\n",
    "            movie_actors[movie].append(row['primaryName'])\n",
    "        else:\n",
    "            movie_actors[movie] = [row['primaryName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_movies = pd.merge(merged_movies, imdb_rating[['tconst', 'averageRating']], on='tconst', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_movies['actors'] = merged_movies['tconst'].map(movie_actors)\n",
    "merged_movies['actors'] = merged_movies['actors'].fillna('').apply(lambda x: ','.join(x[:5]) if isinstance(x, list) else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the features column by concatenating the genres, actors and actresses columns. We then use the TfidfVectorizer to transform the features into a matrix of token counts. We use **TF-IDF** to prepare the data for the recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_movie_features(movie):\n",
    "    features = []\n",
    "    for genre in ['Action', 'Adventure', 'Animation', 'Biography', \"Children's\", 'Comedy', 'Crime', \n",
    "                  'Documentary', 'Drama', 'Family', 'Fantasy', 'Film-Noir', 'Game-Show', 'History', \n",
    "                  'Horror', 'Music', 'Musical', 'Mystery', 'News', 'Reality-TV', 'Romance', 'Sci-Fi', \n",
    "                  'Short', 'Sport', 'Talk-Show', 'Thriller', 'War', 'Western', 'Adult']:\n",
    "        if movie[genre] == 1:\n",
    "            features.append(genre)\n",
    "    features.extend(movie['actors'].split(','))\n",
    "    return ' '.join(features)\n",
    "\n",
    "merged_movies['features'] = merged_movies.apply(get_movie_features, axis=1)\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(merged_movies['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "user_item_matrix = ml_ratings.pivot(index='movie_id', columns='user_id', values='rating').fillna(0)\n",
    "csr_ratings = csr_matrix(user_item_matrix.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and recommendation\n",
    "\n",
    "I decided to recommend 3 movies based on my own experience I like to still have the choice over what I will watch. Movies will be recommend based on features and users rating and then order by the average rating from IMDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "model = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=20, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach is very naive. We will combine the content-based filtering and collaborative filtering.\n",
    "\n",
    "- `get_movie_recommendations`: This function uses collaborative filtering to find similar movies based on user ratings.\n",
    "- `make_recommendations`: This is the main function that combines content-based and collaborative filtering to generate recommendations. This is were we combine the features extracted from the IMDB dataset and the user ratings from the MovieLens 1M dataset.\n",
    "\n",
    "In details: The function `make_recommendations(movie1, movie2)` first retrieves the specified movies from the dataset. It then combines their TF-IDF vectors to create a unified feature vector. Using this combined vector, it identifies the top 20 similar movies through content-based recommendations. Concurrently, it obtains recommendations via collaborative filtering for each movie. These recommendations are merged, filtered, and sorted based on frequency and rating. Finally, the function outputs the top 3 recommended movie titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "\n",
    "def get_movie_recommendations(movie_id, user_item_matrix, model, n_recommendations=5):\n",
    "    if movie_id not in user_item_matrix.index:\n",
    "        return []\n",
    "    movie_vector = user_item_matrix.loc[movie_id].values.reshape(1, -1)\n",
    "    _, indices = model.kneighbors(movie_vector, n_neighbors=n_recommendations+1)\n",
    "    similar_movies = user_item_matrix.index[indices.flatten()][1:]\n",
    "    return similar_movies.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recommendations(movie1, movie2):\n",
    "    model.fit(csr_ratings)\n",
    "    \n",
    "    movies = merged_movies[merged_movies['primary_title'].isin([movie1, movie2])]\n",
    "    if len(movies) != 2:\n",
    "        raise ValueError(\"One or both movies not found in the dataset\")\n",
    "    \n",
    "    indices = movies.index\n",
    "    combined_features = tfidf_matrix[indices[0]] + tfidf_matrix[indices[1]]\n",
    "    similar_scores = cosine_similarity(combined_features, tfidf_matrix).flatten()\n",
    "    \n",
    "    content_based_indices = similar_scores.argsort()[::-1][2:22]  # Top 20 excluding input movies\n",
    "    content_based_recommendations = merged_movies.iloc[content_based_indices]['movie_id'].tolist()\n",
    "    \n",
    "    collaborative_recommendations = []\n",
    "    for movie_id in movies['movie_id']:\n",
    "        collaborative_recommendations.extend(get_movie_recommendations(movie_id, user_item_matrix, model))\n",
    "    \n",
    "    # Combine content-based and collaborative recommendations\n",
    "    all_recommendations = content_based_recommendations + collaborative_recommendations\n",
    "    recommendation_counts = Counter(all_recommendations)\n",
    "    \n",
    "    valid_recommendations = [\n",
    "        (movie_id, (count, merged_movies.loc[merged_movies['movie_id'] == movie_id, 'averageRating'].iloc[0]))\n",
    "        for movie_id, count in recommendation_counts.items()\n",
    "        if movie_id in merged_movies['movie_id'].values\n",
    "    ]\n",
    "    \n",
    "    sorted_recommendations = sorted(valid_recommendations, key=lambda x: (x[1][0], x[1][1]), reverse=True)\n",
    "    \n",
    "    # Return top 3 recommendations\n",
    "    top_recommendations = sorted_recommendations[:3]\n",
    "    return [merged_movies.loc[merged_movies['movie_id'] == rec[0], 'primary_title'].iloc[0] for rec in top_recommendations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie1 = 'Toy Story'\n",
    "movie2 = 'Jumanji'\n",
    "\n",
    "recommendations = make_recommendations(movie1, movie2)\n",
    "print(f\"Recommendations for {movie1} and {movie2}:\")\n",
    "for i, rec in enumerate(recommendations):\n",
    "    print(f\"{i+1}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The current method combines content-based and collaborative recommendations by simply adding them together and then ranking based on frequency and average rating. Let's break down the advantages and disadvantages of this approach.\n",
    "\n",
    "**Advantages:**\n",
    "- The model is simple and easy to understand.\n",
    "- Easy to implement.\n",
    "- It still uses both content-based and collaborative filtering.\n",
    "\n",
    "**Disadvantages:**\n",
    "- It does not directly balance the influence of content-based and collaborative filtering.\n",
    "- It might favor movies that appear in both content-based and collaborative recommendations.\n",
    "\n",
    "We might improve this model by adding **weights** to the content-based and collaborative recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `make_recommendations_improved(movie1, movie2, content_weight=0.2, collab_weight=0.4, movies_df=merged_movies)` first retrieves the specified movies from the dataset. It combines their TF-IDF vectors to create a unified feature vector and calculates content-based similarity scores. These scores are normalized for consistency. Concurrently, it computes collaborative filtering scores by recommending similar movies for each input movie, with scores normalized similarly. The function then combines the normalized content and collaborative scores using the provided weights. After excluding the input movies from the recommendations, it identifies the top 10 recommendations based on the combined scores. These recommendations are sorted by the combined score and average rating. Finally, the function outputs the top 3 recommended movie titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_recommendations_improved(movie1, movie2, content_weight=0.2, collab_weight=0.4, movies_df=merged_movies):\n",
    "    model.fit(csr_ratings)\n",
    "    \n",
    "    movies = movies_df[movies_df['primary_title'].isin([movie1, movie2])]\n",
    "    if len(movies) != 2:\n",
    "        raise ValueError(\"One or both movies not found in the dataset\")\n",
    "    \n",
    "    indices = movies.index\n",
    "    combined_features = tfidf_matrix[indices[0]] + tfidf_matrix[indices[1]]\n",
    "    content_scores = cosine_similarity(combined_features, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Normalize content scores\n",
    "    content_scores = (content_scores - content_scores.min()) / (content_scores.max() - content_scores.min())\n",
    "    \n",
    "    # Get collaborative scores\n",
    "    collab_scores = np.zeros(len(movies_df))\n",
    "    for movie_id in movies['movie_id']:\n",
    "        similar_movies = get_movie_recommendations(movie_id, user_item_matrix, model, n_recommendations=len(movies_df))\n",
    "        for i, similar_movie in enumerate(similar_movies):\n",
    "            collab_scores[movies_df.index[movies_df['movie_id'] == similar_movie]] += 1 / (i + 1)\n",
    "    \n",
    "    # Normalize collaborative scores\n",
    "    collab_scores = (collab_scores - collab_scores.min()) / (collab_scores.max() - collab_scores.min())\n",
    "    \n",
    "    # Combine scores\n",
    "    combined_scores = content_weight * content_scores + collab_weight * collab_scores\n",
    "    \n",
    "    # Remove input movies from recommendations\n",
    "    combined_scores[indices] = -1\n",
    "    \n",
    "    # Get top recommendations\n",
    "    top_indices = combined_scores.argsort()[::-1][:10]  # Get top 10 recommendations\n",
    "    \n",
    "    recommendations = []\n",
    "    for idx in top_indices:\n",
    "        movie = movies_df.iloc[idx]\n",
    "        recommendations.append((movie['primary_title'], movie['averageRating'], combined_scores[idx]))\n",
    "    \n",
    "    # Sort by combined score and then by average rating\n",
    "    recommendations.sort(key=lambda x: (x[2], x[1]), reverse=True)\n",
    "    \n",
    "    return [rec[0] for rec in recommendations[:3]]  # Return top 3 recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie1 = 'Toy Story'\n",
    "movie2 = 'Jumanji'\n",
    "\n",
    "recommendations = make_recommendations_improved(movie1, movie2)\n",
    "print(f\"Recommendations for {movie1} and {movie2}:\")\n",
    "for i, rec in enumerate(recommendations):\n",
    "    print(f\"{i+1}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This improved version:\n",
    "\n",
    "- Uses both content-based and collaborative methods for all potential recommendations.\n",
    "- Normalizes scores from both methods to ensure fair comparison.\n",
    "- Allows adjusting the weight of content-based vs. collaborative methods.\n",
    "- Provides a more nuanced ranking system that considers both similarity and user ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misunderstanding of the task\n",
    "\n",
    "I thought we had to predict movies based on 2 movies input but we have to provide 2 users. The function make recommendations has to be adapted to take 2 users. \n",
    "\n",
    "New version of the function `make_recommendations_improved`:\n",
    "- Retrieves the movies rated by each user\n",
    "- Combines these movies to create a joint user profile\n",
    "- Calculates an average feature vector for their combined movies\n",
    "- Computes content-based similarity scores using this combined vector\n",
    "- Determines collaborative filtering scores based on similar movies\n",
    "- Merges content-based and collaborative scores with adjustable weights\n",
    "- Excludes movies that either user has already rated\n",
    "- Ranks potential recommendations based on the combined scores and average ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def make_recommendations_for_users(user1_id, user2_id, content_weight=0.2, collab_weight=0.4, movies_df=merged_movies, n_recommendations=5):\n",
    "    model.fit(csr_ratings)\n",
    "    \n",
    "    # Get the movies rated by each user\n",
    "    user1_movies = ml_ratings[ml_ratings['user_id'] == user1_id]['movie_id'].tolist()\n",
    "    user2_movies = ml_ratings[ml_ratings['user_id'] == user2_id]['movie_id'].tolist()\n",
    "    \n",
    "    # Combine the movies from both users\n",
    "    combined_movies = list(set(user1_movies + user2_movies))\n",
    "    \n",
    "    if len(combined_movies) == 0:\n",
    "        raise ValueError(\"No movies found for these users\")\n",
    "    \n",
    "    # Calculate the average feature vector for the combined movies\n",
    "    combined_features = tfidf_matrix[movies_df[movies_df['movie_id'].isin(combined_movies)].index].mean(axis=0)\n",
    "    combined_features = np.asarray(combined_features).flatten()  # Convert to 1D numpy array\n",
    "    \n",
    "    content_scores = cosine_similarity(combined_features.reshape(1, -1), tfidf_matrix).flatten()\n",
    "    \n",
    "    # Normalize content scores\n",
    "    content_scores = (content_scores - content_scores.min()) / (content_scores.max() - content_scores.min())\n",
    "    \n",
    "    # Get collaborative scores\n",
    "    collab_scores = np.zeros(len(movies_df))\n",
    "    for movie_id in combined_movies:\n",
    "        similar_movies = get_movie_recommendations(movie_id, user_item_matrix, model, n_recommendations=len(movies_df))\n",
    "        for i, similar_movie in enumerate(similar_movies):\n",
    "            idx = movies_df.index[movies_df['movie_id'] == similar_movie]\n",
    "            if len(idx) > 0:\n",
    "                collab_scores[idx[0]] += 1 / (i + 1)\n",
    "    \n",
    "    # Normalize collaborative scores\n",
    "    if collab_scores.max() != collab_scores.min():\n",
    "        collab_scores = (collab_scores - collab_scores.min()) / (collab_scores.max() - collab_scores.min())\n",
    "    else:\n",
    "        collab_scores = np.zeros_like(collab_scores)\n",
    "    \n",
    "    # Combine scores\n",
    "    combined_scores = content_weight * content_scores + collab_weight * collab_scores\n",
    "    \n",
    "    # Remove already watched movies from recommendations\n",
    "    combined_scores[movies_df[movies_df['movie_id'].isin(combined_movies)].index] = -1\n",
    "    \n",
    "    # Get top recommendations\n",
    "    top_indices = combined_scores.argsort()[::-1][:10]  # Get top 10 recommendations\n",
    "    \n",
    "    recommendations = []\n",
    "    for idx in top_indices:\n",
    "        movie = movies_df.iloc[idx]\n",
    "        recommendations.append((movie['primary_title'], movie['averageRating'], combined_scores[idx]))\n",
    "    \n",
    "    # Sort by combined score and then by average rating\n",
    "    recommendations.sort(key=lambda x: (x[2], x[1]), reverse=True)\n",
    "    \n",
    "    return [rec[0] for rec in recommendations[:n_recommendations]]  # Return top n recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print user ids\n",
    "print(\"User IDs:\")\n",
    "print(ml_ratings['user_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user1_id = 'user_123'\n",
    "user2_id = 'user_6'\n",
    "\n",
    "recommendations = make_recommendations_for_users(user1_id, user2_id)\n",
    "print(f\"Recommendations for users {user1_id} and {user2_id}:\")\n",
    "for i, rec in enumerate(recommendations):\n",
    "    print(f\"{i+1}. {rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check is the users have already seen the recommended movies\n",
    "user1_movies = ml_ratings[ml_ratings['user_id'] == user1_id]['movie_id'].tolist()\n",
    "user2_movies = ml_ratings[ml_ratings['user_id'] == user2_id]['movie_id'].tolist()\n",
    "\n",
    "print(f\"Movies seen by {user1_id}:\")\n",
    "print(user1_movies)\n",
    "print(f\"Movies seen by {user2_id}:\")\n",
    "print(user2_movies)\n",
    "\n",
    "# Check if the recommended movies are in the list of movies seen by the users\n",
    "print(\"Recommended movies:\")\n",
    "print(recommendations)\n",
    "\n",
    "recommendations_id = merged_movies[merged_movies['primary_title'].isin(recommendations)]['movie_id'].tolist()\n",
    "print(\"Recommended movies IDs:\")\n",
    "print(recommendations_id)\n",
    "\n",
    "print(\"Recommended movies already seen by the users:\")\n",
    "print(set(recommendations_id).intersection(user1_movies))\n",
    "print(set(recommendations_id).intersection(user2_movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The recommendation system is based on a combination of content-based and collaborative filtering. It uses TF-IDF vectors to represent movie features and user ratings to identify similar movies. The model can be improved by adjusting the weights of content-based and collaborative recommendations. The final recommendations are based on the combined scores and average ratings.\n",
    "\n",
    "The system is simple and easy to understand, providing a good starting point for movie recommendations. Further enhancements could involve more sophisticated algorithms and additional features to improve accuracy and personalization. The current model is quite slow and may not scale well to larger datasets. Optimizations such as vectorization and parallel processing could be explored to improve performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
